{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network-Based Stock Recommendation System**\n",
        "\n",
        "\n",
        "- Combines financial data and news sentiment to rank S&P 500 stocks.\n",
        "\n",
        "- Uses a Bidirectional LSTM model to evaluate financial performance over time.\n",
        "\n",
        "- Uses FinBERT (Transformer) to classify sentiment from real news articles.\n",
        "\n",
        "- Final recommendation score = 80% financial_score + 20% sentiment_score.\n",
        "\n",
        "- Risk is assessed using debt ratios and downside volatility.\n",
        "\n",
        "- Interactive Gradio UI filters top stocks by sector and risk level.\n",
        "\n",
        "- Designed for scalability and real-world adaptability with minimal changes.\n",
        "\n",
        "All code runs end-to-end inside this notebook using local files\n",
        "Includes visualizations, metrics, and a user-friendly recommender UI\n",
        "\n",
        "\n",
        "**Note: Restart and run all cells command causes session to time out. please run each cell individually**\n",
        "\n"
      ],
      "metadata": {
        "id": "bhvj3Dwekk1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies:**  \n",
        "Installs required libraries like NumPy, pandas, TensorFlow, etc.\n",
        "\n",
        "Note: You will have to restart your runtime in order for these changes to take effect after running this cell.\n"
      ],
      "metadata": {
        "id": "7S91NUTzbdUO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWVPS0YVbEFm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "!pip install -U  \\\n",
        "    numpy==1.26.4 \\\n",
        "    pandas==2.2.2 \\\n",
        "    scikit-learn==1.6.1 \\\n",
        "    matplotlib==3.8.4 \\\n",
        "    seaborn==0.13.2 \\\n",
        "    tensorflow==2.16.1 \\\n",
        "    tensorflow-text==2.16.1 \\\n",
        "    transformers==4.39.3 \\\n",
        "    torch==2.1.2 \\\n",
        "    optuna==4.2.0 \\\n",
        "    yfinance==0.2.55 \\\n",
        "    datasets \\\n",
        "    rapidfuzz \\\n",
        "    gradio\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unzip and Load Dataset Files**\n",
        "\n",
        "This cell extracts the financial and sentiment dataset ZIP files and lists their contents. Ensure the files are named financial.zip and senti.zip before uploading.\n",
        "\n",
        "Note: If you get this error:  ValueError: numpy.dtype size changed, may indicate binary incompatibility.\n",
        "\n",
        "If you get this error, please restart your runtime."
      ],
      "metadata": {
        "id": "4Qi2Ye0mb7xn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By72KezHfmsD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "financial_zip_path = \"/content/financial.zip\"\n",
        "sentiment_zip_path = \"/content/senti.zip\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_zip(file_path, extract_to):\n",
        "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        print(f\"Extracted: {file_path} â†’ {extract_to}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"{file_path} not found. Make sure it's in your Google Drive.\")\n",
        "\n",
        "\n",
        "extract_zip(financial_zip_path, \"financial_data\")\n",
        "extract_zip(sentiment_zip_path, \"sentiment_data\")\n",
        "\n",
        "\n",
        "print(\"\\nFinancial Data Files:\", os.listdir(\"financial_data\"))\n",
        "print(\"\\nSentiment Data Files:\", os.listdir(\"sentiment_data\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Combine Yearly Financial CSVs**\n",
        "\n",
        "This cell reads all yearly financial CSV files, adds a Year column extracted from the filenames, standardizes the Ticker column, and concatenates everything into a single DataFrame called combined_df."
      ],
      "metadata": {
        "id": "f4phR7coc2hw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3zL-ZpZUkif"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "folder_path = \"financial_data\"\n",
        "all_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".csv\")])\n",
        "\n",
        "def extract_year(filename):\n",
        "    return int(filename.split(\"_\")[0])\n",
        "\n",
        "all_dfs = []\n",
        "\n",
        "for file in all_files:\n",
        "    year = extract_year(file)\n",
        "    df = pd.read_csv(os.path.join(folder_path, file))\n",
        "\n",
        "\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        df = df.rename(columns={'Unnamed: 0': 'Ticker'})\n",
        "    elif df.index.name == 'Ticker':\n",
        "        df.reset_index(inplace=True)\n",
        "    else:\n",
        "        df['Ticker'] = df.index\n",
        "\n",
        "    df['Year'] = year\n",
        "    all_dfs.append(df)\n",
        "\n",
        "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "print(f\"Combined shape: {combined_df.shape}\")\n",
        "combined_df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess Financial Data for LSTM Model**\n",
        "\n",
        "This cell filters the combined financial dataset to retain only rows with all required features (Class + 16 financial indicators). It then selects only those tickers that have a full 5-year time series of data, resulting in a clean df_lstm dataset ready for LSTM training.\n",
        "\n"
      ],
      "metadata": {
        "id": "4BqB3YcmdMSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg4usWoLdxLc"
      },
      "outputs": [],
      "source": [
        "\n",
        "combined_df = combined_df.sort_values(by=[\"Ticker\", \"Year\"])\n",
        "\n",
        "\n",
        "features = [\n",
        "    'Revenue', 'Revenue Growth', 'Gross Profit', 'Operating Income',\n",
        "    'Net Income', 'EPS', 'EBIT Margin', 'Profit Margin', 'EBITDA',\n",
        "    'Operating Cash Flow', 'Market Cap', 'Enterprise Value',\n",
        "    'Debt to Equity', 'Interest Coverage', 'ROIC', 'Net Profit Margin'\n",
        "]\n",
        "\n",
        "\n",
        "required_columns = ['Ticker', 'Year', 'Class'] + features\n",
        "df_lstm = combined_df[required_columns].dropna()\n",
        "\n",
        "\n",
        "grouped = df_lstm.groupby('Ticker')\n",
        "\n",
        "\n",
        "sequence_length = 5\n",
        "valid_tickers = [name for name, group in grouped if len(group) == sequence_length]\n",
        "\n",
        "\n",
        "df_lstm = df_lstm[df_lstm['Ticker'].isin(valid_tickers)]\n",
        "\n",
        "print(f\"LSTM dataset shape (after filtering): {df_lstm.shape}\")\n",
        "print(f\"Number of valid time series (tickers with 5 years): {len(valid_tickers)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalize & Format Time Series Data for LSTM Training**\n",
        "\n",
        "This cell prepares the filtered dataset for LSTM model training:\n",
        "\n",
        "- The selected financial features are normalized using StandardScaler to ensure uniform input scale across all features.\n",
        "\n",
        "- For each ticker with 5 years of data, we extract a sequence of shape (5, num_features) and assign the final year's class label as the target.\n",
        "\n",
        "- The result is a 3D input array X for LSTM training and a 1D label array y, suitable for supervised classification."
      ],
      "metadata": {
        "id": "OHevkJuFeH4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQPNq7YIeLmk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "sequences = []\n",
        "labels = []\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_lstm.loc[:, features] = scaler.fit_transform(df_lstm[features])\n",
        "\n",
        "\n",
        "\n",
        "for ticker, group in df_lstm.groupby(\"Ticker\"):\n",
        "    group = group.sort_values(\"Year\")\n",
        "    if len(group) == sequence_length:\n",
        "        seq = group[features].values\n",
        "        label = group[\"Class\"].values[-1]\n",
        "        sequences.append(seq)\n",
        "        labels.append(label)\n",
        "\n",
        "\n",
        "X = np.array(sequences)\n",
        "y = np.array(labels)\n",
        "\n",
        "print(f\"Final LSTM Input Shape: {X.shape}\")\n",
        "print(f\"Final Labels Shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Financial LSTM Model Training with Bidirectional Memory and Attention**\n",
        "\n",
        "This cell trains a custom neural network to predict long-term stock potential using time-series financial data:\n",
        "\n",
        "- We use a Bidirectional LSTM, allowing the model to learn from past and future trends across 5 years of data per stock.\n",
        "\n",
        "- An Attention layer is applied to dynamically focus on the most informative years in the sequence, improving interpretability and prediction quality.\n",
        "\n",
        "- Class imbalance (due to fewer failing companies) is corrected using manual class weights ({0: 1.25, 1: 0.83}), boosting sensitivity to underrepresented outcomes.\n",
        "\n",
        "- EarlyStopping halts training when validation loss plateaus, reducing overfitting.\n",
        "\n",
        "- After training, we report final test accuracy and a classification report showing performance on unseen stocks."
      ],
      "metadata": {
        "id": "Coc2yvSAecAn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx264HRS6DT3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional, Attention, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "\n",
        "class_weights = {0: 1.25, 1: 0.83}\n",
        "\n",
        "\n",
        "input_layer = Input(shape=(X.shape[1], X.shape[2]), name=\"input_layer\")\n",
        "x = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Attention()([x, x])\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "output = Dense(1, activation='sigmoid', name=\"dense_output\")(x)\n",
        "\n",
        "finmodel = Model(inputs=input_layer, outputs=output)\n",
        "finmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "finmodel.summary()\n",
        "\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = finmodel.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "test_loss, test_acc = finmodel.evaluate(X_test, y_test)\n",
        "print(f\"\\n Final Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "y_pred = (finmodel.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM Financial Model: Performance Visualization**\n",
        "\n",
        "This section provides key visualizations to evaluate the performance of the LSTM-based financial classifier:\n",
        "\n",
        "- Training Curves: Accuracy and loss trends over epochs to verify convergence and spot overfitting.\n",
        "\n",
        "- Confusion Matrix: Shows how well the model distinguishes between high and low performers.\n",
        "\n",
        "- ROC Curve: Illustrates the model's ability to balance true positives vs. false positives, with AUC as a summary metric.\n",
        "\n",
        "These plots offer a holistic view of the modelâ€™s behavior and predictive performance on the test set."
      ],
      "metadata": {
        "id": "yAw-W14zrCTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_df['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_df['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy (Financial Model)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_df['loss'], label='Train Loss')\n",
        "plt.plot(history_df['val_loss'], label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss (Financial Model)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not High Performer\", \"High Performer\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\" Confusion Matrix â€“ LSTM Financial Model\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "y_probs = finmodel.predict(X_test).ravel()\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([-0.01, 1.01])\n",
        "plt.ylim([-0.01, 1.01])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\" ROC Curve â€“ LSTM Financial Model\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z4gGwB-6okdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Preview Sentiment Training & Validation Data**\n",
        "\n",
        "This cell loads the pre-labeled sentiment datasets for training and validation. Each row contains a financial news sentence and its associated sentiment label (0 = negative, 1 = neutral, 2 = positive).\n",
        "\n",
        "We preview the structure of the data and display the distribution of sentiment classes to understand any class imbalance."
      ],
      "metadata": {
        "id": "IZyxUK6TfkS8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7skFk3D9NLH"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_df = pd.read_csv(\"sentiment_data/sent_train.csv\")\n",
        "valid_df = pd.read_csv(\"sentiment_data/sent_valid.csv\")\n",
        "\n",
        "\n",
        "print(\"Training Sample:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nValidation Sample:\")\n",
        "print(valid_df.head())\n",
        "\n",
        "\n",
        "print(\"\\nColumns in train set:\", train_df.columns)\n",
        "print(\"\\nLabel Distribution (Train):\")\n",
        "print(train_df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Sentiment Data into Hugging Face Format**\n",
        "\n",
        "We use the datasets library from Hugging Face to load the sentiment training and validation CSVs into a structured DatasetDict.\n",
        "\n",
        "This allows for cleaner integration with Transformers pipelines and batch processing for tokenization and training."
      ],
      "metadata": {
        "id": "0WyyDiuPgBsH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWDWYLInLt1o"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "data_files = {\n",
        "    \"train\": \"sentiment_data/sent_train.csv\",\n",
        "    \"validation\": \"sentiment_data/sent_valid.csv\"\n",
        "}\n",
        "\n",
        "raw_datasets = load_dataset(\"csv\", data_files=data_files)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdQrCJ9IFYlm"
      },
      "source": [
        "**Sentiment Classification Using FinBERT (Transformer-Based Model)**\n",
        "\n",
        "In this section, we fine-tune the ProsusAI/finbert model to classify financial sentiment into three classes.\n",
        "Steps performed:\n",
        "\n",
        "- Loaded the FinBERT tokenizer and model (frozen to speed up training).\n",
        "\n",
        "- Tokenized sentiment text from the dataset (padding and truncation to 128 tokens).\n",
        "\n",
        "- Built a Keras model that extracts the [CLS] embedding for sentiment representation.\n",
        "\n",
        "- Added dense layers for classification with dropout for regularization.\n",
        "\n",
        "- Trained the model using labeled sentiment data for 3 epochs.\n",
        "\n",
        "- Evaluated the model on a held-out test set and reported accuracy.\n",
        "\n",
        "Note: If you get a ValueError during training, restart the runtime to fix TensorFlow graph conflicts.\n",
        "\n",
        "If you get this error, please restart your runtime.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfTqFbIiO-94"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.saving import register_keras_serializable\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "checkpoint = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "bert_model = TFBertModel.from_pretrained(checkpoint)\n",
        "bert_model.trainable = False\n",
        "\n",
        "def tokenize_data(df, tokenizer, max_len=128):\n",
        "    return tokenizer(\n",
        "        list(df['text']),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "\n",
        "train_full, test_df = train_test_split(train_df, test_size=0.10, stratify=train_df['label'], random_state=42)\n",
        "train_df_small, val_df = train_test_split(train_full, test_size=0.10, stratify=train_full['label'], random_state=42)\n",
        "\n",
        "X_train = tokenize_data(train_df_small, tokenizer)\n",
        "X_val = tokenize_data(val_df, tokenizer)\n",
        "X_test = tokenize_data(test_df, tokenizer)\n",
        "\n",
        "y_train = tf.convert_to_tensor(train_df_small['label'].values)\n",
        "y_val = tf.convert_to_tensor(val_df['label'].values)\n",
        "y_test = tf.convert_to_tensor(test_df['label'].values)\n",
        "\n",
        "\n",
        "@register_keras_serializable()\n",
        "def extract_cls(inputs):\n",
        "    ids, mask = inputs\n",
        "    return bert_model(ids, attention_mask=mask).last_hidden_state[:, 0]\n",
        "\n",
        "\n",
        "input_ids = Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
        "cls_token = Lambda(extract_cls, output_shape=(768,), name=\"cls_extraction\")([input_ids, attention_mask])\n",
        "\n",
        "\n",
        "x = Dense(128, activation='relu')(cls_token)\n",
        "x = Dropout(0.2)(x)\n",
        "output = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    {\"input_ids\": X_train['input_ids'], \"attention_mask\": X_train['attention_mask']},\n",
        "    y_train,\n",
        "    validation_data=(\n",
        "        {\"input_ids\": X_val['input_ids'], \"attention_mask\": X_val['attention_mask']}, y_val\n",
        "    ),\n",
        "    epochs=3,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "\n",
        "loss, acc = model.evaluate(\n",
        "    {\"input_ids\": X_test['input_ids'], \"attention_mask\": X_test['attention_mask']},\n",
        "    y_test\n",
        ")\n",
        "print(f\"\\n Final Test Accuracy: {acc:.4f} â€” this will be used in portfolio scoring.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Model: Performance Visualization\n",
        " This section visualizes the training and evaluation results of the sentiment classification model (FinBERT-based):\n",
        "\n",
        "- Training Curves: Accuracy and loss trends during fine-tuning to assess learning progress and potential overfitting.\n",
        "\n",
        "- Confusion Matrix: Displays prediction accuracy across all sentiment classes: Negative, Neutral, and Positive.\n",
        "\n",
        "- ROC Curve (Micro-Averaged): Aggregates all class performance to show the modelâ€™s overall discriminative ability.\n",
        "\n",
        "- Classification Report: Includes precision, recall, and F1-scores for each class to give a detailed breakdown of performance.\n",
        "\n",
        "These diagnostics demonstrate how well the model classifies financial sentiment based on textual news and updates"
      ],
      "metadata": {
        "id": "i6lYCvt9rAow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_df['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_df['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy (Sentiment Model)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_df['loss'], label='Train Loss')\n",
        "plt.plot(history_df['val_loss'], label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss (Sentiment Model)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "y_pred = model.predict(\n",
        "    {\"input_ids\": X_test['input_ids'], \"attention_mask\": X_test['attention_mask']}\n",
        ").argmax(axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\" Confusion Matrix â€“ Sentiment Model\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "y_probs = model.predict({\"input_ids\": X_test['input_ids'], \"attention_mask\": X_test['attention_mask']})\n",
        "y_test_onehot = tf.one_hot(y_test, depth=3).numpy()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test_onehot.ravel(), y_probs.ravel())\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([-0.01, 1.01])\n",
        "plt.ylim([-0.01, 1.01])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\" ROC Curve â€“ Sentiment Model (Micro-Average)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n"
      ],
      "metadata": {
        "id": "lIS2YlOHp3DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapping Ticker Symbols to Company Names (Using yfinance)**\n",
        "\n",
        "This cell retrieves official company names corresponding to the stock tickers used in our LSTM dataset.\n",
        "\n",
        "- Extracted unique tickers from df_lstm.\n",
        "\n",
        "- Queried Yahoo Finance API via yfinance to fetch each company's shortName.\n",
        "\n",
        "- Successfully mapped most tickers to names and stored them in a dictionary (ticker_to_name).\n",
        "\n",
        "- Logged and counted any failed requests (e.g., due to missing or malformed data).\n",
        "\n",
        "- Added a slight delay between requests to avoid rate-limiting."
      ],
      "metadata": {
        "id": "4T3_OrUAhIfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "\n",
        "\n",
        "logging.getLogger(\"yfinance\").setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "tickers = df_lstm['Ticker'].dropna().unique()\n",
        "\n",
        "ticker_to_name = {}\n",
        "failed = []\n",
        "\n",
        "\n",
        "print(f\" Fetching company names for {len(tickers)} LSTM tickers...\\n\")\n",
        "\n",
        "for i, ticker in enumerate(tickers):\n",
        "    try:\n",
        "        info = yf.Ticker(ticker).info\n",
        "        name = info.get('shortName', '')\n",
        "        if name:\n",
        "            ticker_to_name[ticker] = name\n",
        "            print(f\" [{i+1}/{len(tickers)}] {ticker}: {name}\")\n",
        "        else:\n",
        "            print(f\"  [{i+1}/{len(tickers)}] {ticker}: No name found.\")\n",
        "        time.sleep(0.25)\n",
        "    except Exception as e:\n",
        "        failed.append(ticker)\n",
        "        print(f\" [{i+1}/{len(tickers)}] {ticker}: Failed ({str(e)[:50]})\")\n",
        "\n",
        "\n",
        "print(f\" Done. Mapped {len(ticker_to_name)} tickers. Skipped {len(failed)} tickers.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CP_T4xzb-qZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matching Sentiment Texts to Stock Tickers**\n",
        "\n",
        "The sentiment dataset we use contains financial news and social media posts, but the company references inside these texts are inconsistent â€” sometimes they appear as ticker symbols (e.g., $AAPL), and other times as full company names (e.g., Apple Inc). To properly align this dataset with our financial LSTM dataset (which is organized by ticker), we need a robust matching system.\n",
        "\n",
        "\n",
        "\n",
        "- Reverse Mapping: We create a dictionary that maps lowercase company names to tickers.\n",
        "\n",
        "- Strict Matching: Detects tickers in the form $TICKER using regex. Finds exact matches of company names inside the text.\n",
        "\n",
        "- Fuzzy Matching: If no exact match is found, uses RapidFuzz to perform similarity matching on company names (â‰¥ 90% match confidence).\n",
        "\n",
        "- Result: A clean ticker column is created, aligning sentiment texts to company tickers.\n",
        "\n",
        "- Final Step: Unmatched rows are dropped to ensure downstream consistency.\n",
        "\n",
        "This process is essential for integrating the sentiment signal with our financial predictions, enabling meaningful scoring and recommendation."
      ],
      "metadata": {
        "id": "dZcTUzqkhQd6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii5yij-0XKuK"
      },
      "outputs": [],
      "source": [
        "from rapidfuzz import fuzz, process\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "name_to_ticker = {v.lower(): k for k, v in ticker_to_name.items()}\n",
        "\n",
        "\n",
        "sentiment_df = pd.read_csv(\"sentiment_data/sent_train.csv\")\n",
        "\n",
        "\n",
        "def strict_match_ticker(text):\n",
        "    text_lower = text.lower()\n",
        "\n",
        "\n",
        "    for ticker in ticker_to_name:\n",
        "        if len(ticker) < 2:\n",
        "            continue\n",
        "        pattern = rf\"\\${re.escape(ticker.lower())}(\\b|[^a-zA-Z])\"\n",
        "        if re.search(pattern, text_lower):\n",
        "            print(f\" Matched by $ticker: {ticker} in â†’ {text[:90]}...\")\n",
        "            return ticker\n",
        "\n",
        "\n",
        "    for name, ticker in name_to_ticker.items():\n",
        "        if f\" {name} \" in f\" {text_lower} \":\n",
        "            print(f\" Matched by exact company name: {name} â†’ {text[:90]}...\")\n",
        "            return ticker\n",
        "\n",
        "\n",
        "    best_match = process.extractOne(text_lower, name_to_ticker.keys(), scorer=fuzz.token_sort_ratio)\n",
        "    if best_match and best_match[1] >= 90:\n",
        "        matched_name = best_match[0]\n",
        "        ticker = name_to_ticker[matched_name]\n",
        "        print(f\" Fuzzy matched '{matched_name}' ({best_match[1]}%) â†’ {ticker} in â†’ {text[:90]}...\")\n",
        "        return ticker\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "sentiment_df['ticker'] = sentiment_df['text'].apply(strict_match_ticker)\n",
        "\n",
        "\n",
        "sentiment_df = sentiment_df.dropna(subset=['ticker'])\n",
        "\n",
        "print(f\"\\n Ticker matching complete: {len(sentiment_df)} entries matched.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Prediction Pipeline: Merging Financial and Sentiment Insights**\n",
        "\n",
        "This cell performs the final stage of our stock scoring system, combining predictions from the LSTM financial model and the FinBERT sentiment model. Hereâ€™s a breakdown of what this does:\n",
        "\n",
        "- Filter Financial Data: We limit the LSTM dataset to tickers that also appear in the sentiment dataset to ensure overlap.\n",
        "\n",
        "- Predict Financial Scores: The pre-trained LSTM (finmodel) processes each stock's 5-year time series of financial metrics and outputs a confidence score (between 0 and 1) indicating expected future performance.\n",
        "\n",
        "- Predict Sentiment Scores: We tokenize the sentiment texts using FinBERT's tokenizer and run them through the trained sentiment model to predict sentiment labels (0, 1, or 2). We then average these predictions per ticker to form a sentiment score.\n",
        "\n",
        "- Merge Scores: For each stock, we combine the two signals:\n",
        "    - final_score = 0.8 * financial_score + 0.2 * sentiment_score\n",
        "\n",
        "    - This weighted average prioritizes financial health while still incorporating sentiment.\n",
        "\n",
        "- Diagnostics: We print summary statistics to show which tickers had complete data from both pipelines and identify any mismatches.\n",
        "\n",
        "-  Output: Displays the Top 10 Recommended Stocks based on the final score â€” those with strong financial fundamentals and supportive sentiment.\n",
        "\n",
        "This step is the culmination of our pipeline and forms the basis of our portfolio recommendation system."
      ],
      "metadata": {
        "id": "4nBe2ZzIiXhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "matched_tickers = sentiment_df['ticker'].unique()\n",
        "filtered_lstm_df = df_lstm[df_lstm['Ticker'].isin(matched_tickers)].copy()\n",
        "print(f\"Filtered financial dataset to {filtered_lstm_df['Ticker'].nunique()} tickers ({len(filtered_lstm_df)} rows)\")\n",
        "\n",
        "\n",
        "sequence_length = 5\n",
        "features = [col for col in filtered_lstm_df.columns if col not in ['Ticker', 'Year', 'Class']]\n",
        "\n",
        "X = []\n",
        "tickers_seq = []\n",
        "\n",
        "for ticker in filtered_lstm_df['Ticker'].unique():\n",
        "    df_ticker = filtered_lstm_df[filtered_lstm_df['Ticker'] == ticker].sort_values('Year')\n",
        "    if len(df_ticker) == sequence_length:\n",
        "        X.append(df_ticker[features].values)\n",
        "        tickers_seq.append(ticker)\n",
        "    else:\n",
        "        print(f\" Skipping {ticker} (has only {len(df_ticker)} rows)\")\n",
        "\n",
        "X = np.array(X)\n",
        "print(f\" Input shape for LSTM: {X.shape}\")\n",
        "\n",
        "\n",
        "financial_scores = finmodel.predict(X).squeeze()\n",
        "print(f\" Sample financial predictions: {financial_scores[:5]}\")\n",
        "\n",
        "lstm_results_df = pd.DataFrame({\n",
        "    'Ticker': tickers_seq,\n",
        "    'financial_score': financial_scores\n",
        "})\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "sentiment_texts = list(sentiment_df['text'])\n",
        "encoded = tokenizer(\n",
        "    sentiment_texts,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "\n",
        "sentiment_preds = model.predict({\n",
        "    \"input_ids\": encoded[\"input_ids\"],\n",
        "    \"attention_mask\": encoded[\"attention_mask\"]\n",
        "}, batch_size=32).argmax(axis=1)\n",
        "\n",
        "\n",
        "sentiment_df['sentiment_score'] = sentiment_preds\n",
        "\n",
        "\n",
        "sentiment_scores_df = sentiment_df.groupby('ticker')['sentiment_score'].mean().reset_index()\n",
        "sentiment_scores_df.rename(columns={'ticker': 'Ticker'}, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "final_df = lstm_results_df.merge(sentiment_scores_df, on='Ticker')\n",
        "final_df['final_score'] = 0.8 * final_df['financial_score'] + 0.2 * final_df['sentiment_score']\n",
        "\n",
        "\n",
        "\n",
        "tickers_in_sentiment = set(sentiment_scores_df['Ticker'])\n",
        "tickers_in_lstm = set(lstm_results_df['Ticker'])\n",
        "\n",
        "common_tickers = tickers_in_sentiment & tickers_in_lstm\n",
        "only_in_sentiment = tickers_in_sentiment - tickers_in_lstm\n",
        "only_in_lstm = tickers_in_lstm - tickers_in_sentiment\n",
        "\n",
        "print(f\"\\n Tickers in both sentiment & LSTM: {len(common_tickers)}\")\n",
        "print(f\" Tickers only in sentiment data: {sorted(list(only_in_sentiment))[:10]}{'...' if len(only_in_sentiment) > 10 else ''}\")\n",
        "print(f\" Tickers only in financial data: {sorted(list(only_in_lstm))[:10]}{'...' if len(only_in_lstm) > 10 else ''}\")\n",
        "\n",
        "\n",
        "\n",
        "top_stocks = final_df.sort_values(by='final_score', ascending=False).head(10)\n",
        "print(\"\\n Top 10 Recommended Stocks:\")\n",
        "print(top_stocks[['Ticker', 'financial_score', 'sentiment_score', 'final_score']])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7MWGgDZZH1Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating Risk Scores and Final Dataset Construction**\n",
        "\n",
        "This cell computes a custom risk score for each stock and integrates it into the final dataset alongside financial and sentiment scores. It also adds sector information to enable sector-based filtering later.\n",
        "\n",
        "- Downside Volatility: We extract all the \"PRICE VAR\" columns and compute downside volatility â€” the standard deviation of only the negative annual price changes â€” for each ticker.\n",
        "\n",
        "- Financial Risk: We calculate the average Debt to Equity and Interest Coverage ratios per ticker. The financial risk score is computed as:\n",
        "   - financial_risk_score = DebtÂ toÂ Equity/1+InterestÂ Coverage\n",
        "\n",
        "\n",
        "\n",
        "- Overall Risk Score: Combines downside volatility and financial risk to give a unified risk_score per stock.\n",
        "\n",
        "- Sector Merge: Adds sector information from the original dataset.\n",
        "\n",
        "- Final DataFrame: Merges all components â€” financial score, sentiment score, risk score, and sector â€” into final_df.\n",
        "\n",
        "Finally, it displays the top 10 highest-scoring stocks, now with complete metadata for risk-aware portfolio construction or sector-specific filtering."
      ],
      "metadata": {
        "id": "t3JL9Z_Ai5ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "price_var_cols = [col for col in combined_df.columns if 'PRICE VAR' in col]\n",
        "\n",
        "\n",
        "price_var_long = combined_df[['Ticker'] + price_var_cols].melt(\n",
        "    id_vars='Ticker',\n",
        "    value_vars=price_var_cols,\n",
        "    var_name='Year',\n",
        "    value_name='Price_Var'\n",
        ")\n",
        "\n",
        "\n",
        "price_var_long = price_var_long.dropna()\n",
        "price_var_long = price_var_long[price_var_long['Price_Var'] < 0]\n",
        "\n",
        "\n",
        "downside_volatility = price_var_long.groupby('Ticker')['Price_Var'].std().reset_index()\n",
        "downside_volatility.rename(columns={'Price_Var': 'downside_volatility'}, inplace=True)\n",
        "\n",
        "combined_df['Debt to Equity'] = combined_df['Debt to Equity'].fillna(combined_df['Debt to Equity'].median())\n",
        "combined_df['Interest Coverage'] = combined_df['Interest Coverage'].fillna(combined_df['Interest Coverage'].median())\n",
        "\n",
        "financial_risk = combined_df.groupby('Ticker').agg({\n",
        "    'Debt to Equity': 'mean',\n",
        "    'Interest Coverage': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "financial_risk['financial_risk_score'] = financial_risk['Debt to Equity'] / (1 + financial_risk['Interest Coverage'])\n",
        "\n",
        "\n",
        "risk_df = downside_volatility.merge(financial_risk[['Ticker', 'financial_risk_score']], on='Ticker', how='outer')\n",
        "risk_df['risk_score'] = risk_df['downside_volatility'].fillna(0) + risk_df['financial_risk_score'].fillna(0)\n",
        "\n",
        "\n",
        "sector_info = combined_df[['Ticker', 'Sector']].drop_duplicates()\n",
        "final_df['Company'] = final_df['Ticker'].map(ticker_to_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "final_df = final_df.merge(sector_info, on='Ticker', how='left')\n",
        "final_df = final_df.merge(risk_df[['Ticker', 'risk_score']], on='Ticker', how='left')\n",
        "\n",
        "\n",
        "final_df = final_df[['Ticker','Company', 'Sector', 'financial_score', 'sentiment_score', 'final_score', 'risk_score']]\n",
        "\n",
        "\n",
        "print(\" Final comprehensive dataset ready:\")\n",
        "display(final_df.sort_values(by='final_score', ascending=False).head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "BBk-ofSRVK9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filter Function for Sector & Risk-Based Stock Selection**\n",
        "\n",
        "This function powers the interactive UI for selecting top stocks based on chosen sectors and risk levels:\n",
        "\n",
        "- Quantile-Based Risk Categorization: The risk_score is split into three categories â€” Low, Medium, and High â€” using the 33rd and 66th percentiles.\n",
        "\n",
        "- Dynamic Filtering: Filters the final_df based on:\n",
        "\n",
        "   - selected_sectors: list of sector names\n",
        "\n",
        "   - selected_risk: one of \"Low\", \"Medium\", or \"High\"\n",
        "\n",
        "- Ranking: Returns the top 10 highest final_score stocks after applying the filters.\n",
        "\n",
        "Used in the frontend to let users choose their investment preferences interactively."
      ],
      "metadata": {
        "id": "gbu-OOcVjs8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_stocks_ui(selected_sectors, selected_risk):\n",
        "    df = final_df.copy()\n",
        "\n",
        "\n",
        "    df = df.dropna(subset=['risk_score'])\n",
        "\n",
        "\n",
        "    q_low = df['risk_score'].quantile(0.33)\n",
        "    q_high = df['risk_score'].quantile(0.66)\n",
        "\n",
        "    def categorize_risk(r):\n",
        "        if r <= q_low:\n",
        "            return 'Low'\n",
        "        elif r <= q_high:\n",
        "            return 'Medium'\n",
        "        else:\n",
        "            return 'High'\n",
        "\n",
        "\n",
        "    df['risk_category'] = df['risk_score'].apply(categorize_risk)\n",
        "\n",
        "\n",
        "    if selected_sectors:\n",
        "        df = df[df['Sector'].isin(selected_sectors)]\n",
        "\n",
        "\n",
        "    if selected_risk:\n",
        "        df = df[df['risk_category'] == selected_risk]\n",
        "\n",
        "\n",
        "    top_df = df.sort_values(by='final_score', ascending=False).head(10)\n",
        "\n",
        "\n",
        "    print(f\" Matches found: {len(top_df)} for sectors {selected_sectors} and risk '{selected_risk}'\")\n",
        "\n",
        "    return top_df[['Ticker','Company', 'Sector', 'financial_score', 'sentiment_score', 'final_score', 'risk_score']]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fJk2CMTQktIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interactive Stock Recommender UI with Gradio**\n",
        "\n",
        "This cell builds an interactive user interface using Gradio:\n",
        "\n",
        "- CheckboxGroup: Lets users select one or more sectors.\n",
        "\n",
        "- Radio Button: Lets users choose a desired risk level â€” Low, Medium, or High.\n",
        "\n",
        "- Output: Displays the top 10 recommended stocks based on final_score after filtering by sector and risk.\n",
        "\n",
        "This makes the model's recommendations visually accessible and easy to explore without needing to rerun any code."
      ],
      "metadata": {
        "id": "swsb1tZXkO0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "sector_list = sorted(final_df['Sector'].dropna().unique().tolist())\n",
        "\n",
        "gr.Interface(\n",
        "    fn=filter_stocks_ui,\n",
        "    inputs=[\n",
        "        gr.CheckboxGroup(choices=sector_list, label=\"Select Sector(s)\"),\n",
        "        gr.Radio(choices=['Low', 'Medium', 'High'], label=\"Select Risk Level\"),\n",
        "    ],\n",
        "    outputs=gr.Dataframe(label=\"Top 10 Matching Stocks\"),\n",
        "    title=\"ðŸ“Š Sector + Risk-Aware Stock Recommender\",\n",
        "    description=\"Choose one or more sectors and a risk level to see high-potential stocks.\"\n",
        ").launch(debug=True)\n"
      ],
      "metadata": {
        "id": "3b5POjWUkvs6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}